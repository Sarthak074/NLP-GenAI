{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIQV76I_ZXW3",
        "outputId": "eda30b6a-68b2-4a1b-b33f-fc13de5168bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.5-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.5/42.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20231228 (from pdfplumber)\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.1.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Downloading pdfplumber-0.11.5-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20231228 pdfplumber-0.11.5 pypdfium2-4.30.1\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "!pip install pdfplumber"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Timport nltk\n",
        "from nltk.tokenize import word_tokenize, WordPunctTokenizer, TreebankWordTokenizer, TweetTokenizer\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import MWETokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import word_tokenize"
      ],
      "metadata": {
        "id": "Eqco7732ZyBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2kImuqSZ1en",
        "outputId": "e9560cb6-6b57-483c-92e5-2ca8e27064e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello! my name is Sarthak Bhandare, Check my tweet on @SarthakBhandare\""
      ],
      "metadata": {
        "id": "8LIXUzS1Z9F4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "whitespace_tokens = text.split()"
      ],
      "metadata": {
        "id": "Ux_0yQGnaDsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordpunct_tokenizer = WordPunctTokenizer()\n",
        "punctuation_tokens = wordpunct_tokenizer.tokenize(text)"
      ],
      "metadata": {
        "id": "edv8b6OuaH8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "treebank_tokenizer = TreebankWordTokenizer()\n",
        "treebank_tokens = treebank_tokenizer.tokenize(text)"
      ],
      "metadata": {
        "id": "03XIJj1faK_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_tokenizer = TweetTokenizer()\n",
        "tweet_tokens = tweet_tokenizer.tokenize(text)"
      ],
      "metadata": {
        "id": "RyHkYrLRaODH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mwe_tokens = ['@user', '#nlp', 'Hello', 'sample', 'text', 'amazing', 'tweets']"
      ],
      "metadata": {
        "id": "Qczew5PRaQvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krm629k-bSiQ",
        "outputId": "6704b19c-d5a6-4dda-ac3e-fb27265e6c58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "porter_stemmer = PorterStemmer()\n",
        "porter_stems = [porter_stemmer.stem(word) for word in word_tokenize(text)]"
      ],
      "metadata": {
        "id": "9ktBU156aTqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "snowball_stemmer = SnowballStemmer(\"english\")\n",
        "snowball_stems = [snowball_stemmer.stem(word) for word in word_tokenize(text)]"
      ],
      "metadata": {
        "id": "VYrR4lGJauu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in word_tokenize(text)]"
      ],
      "metadata": {
        "id": "RpeX3GUJbXj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Whitespace Tokenization:\", whitespace_tokens)\n",
        "print(\"Punctuation-based Tokenization:\", punctuation_tokens)\n",
        "print(\"Treebank Tokenization:\", treebank_tokens)\n",
        "print(\"Tweet Tokenization:\", tweet_tokens)\n",
        "print(\"MWE Tokenization:\", mwe_tokens)\n",
        "print(\"Porter Stemming:\", porter_stems)\n",
        "print(\"Snowball Stemming:\", snowball_stems)\n",
        "print(\"Lemmatized Tokens:\", lemmatized_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXmkUsE2bZ5_",
        "outputId": "2babaa66-d77b-46cd-9a2f-296f118d6222"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Whitespace Tokenization: ['Hello!', 'my', 'name', 'is', 'Sarthak', 'Bhandare,', 'Check', 'my', 'tweet', 'on', '@SarthakBhandare']\n",
            "Punctuation-based Tokenization: ['Hello', '!', 'my', 'name', 'is', 'Sarthak', 'Bhandare', ',', 'Check', 'my', 'tweet', 'on', '@', 'SarthakBhandare']\n",
            "Treebank Tokenization: ['Hello', '!', 'my', 'name', 'is', 'Sarthak', 'Bhandare', ',', 'Check', 'my', 'tweet', 'on', '@', 'SarthakBhandare']\n",
            "Tweet Tokenization: ['Hello', '!', 'my', 'name', 'is', 'Sarthak', 'Bhandare', ',', 'Check', 'my', 'tweet', 'on', '@SarthakBhandare']\n",
            "MWE Tokenization: ['@user', '#nlp', 'Hello', 'sample', 'text', 'amazing', 'tweets']\n",
            "Porter Stemming: ['hello', '!', 'my', 'name', 'is', 'sarthak', 'bhandar', ',', 'check', 'my', 'tweet', 'on', '@', 'sarthakbhandar']\n",
            "Snowball Stemming: ['hello', '!', 'my', 'name', 'is', 'sarthak', 'bhandar', ',', 'check', 'my', 'tweet', 'on', '@', 'sarthakbhandar']\n",
            "Lemmatized Tokens: ['Hello', '!', 'my', 'name', 'is', 'Sarthak', 'Bhandare', ',', 'Check', 'my', 'tweet', 'on', '@', 'SarthakBhandare']\n"
          ]
        }
      ]
    }
  ]
}